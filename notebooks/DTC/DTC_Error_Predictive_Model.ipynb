{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/parth/Machine Learning/Datasets/North Corp/DTC'\n",
    "filePcode = \"Top_LMD_DTC_SPN.csv\"\n",
    "file_list = os.listdir(base_dir)\n",
    "file_list_corrected = []\n",
    "for file_ in file_list:\n",
    "    if file_.endswith(\".csv\") and file_ != filePcode:\n",
    "        file_list_corrected.append(file_)\n",
    "file_list = file_list_corrected\n",
    "del file_list_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "filename = os.path.join(base_dir , file_list[0])\n",
    "data = pd.read_csv(filename)\n",
    "pcode_data = pd.read_csv(os.path.join(base_dir , filePcode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = data.deviceID.unique()\n",
    "device_data = []\n",
    "for id_ in ids:\n",
    "    device_data.append(data[data.deviceID == id_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , device_data_ in enumerate(device_data):\n",
    "    device_data_ = device_data_.reset_index()\n",
    "    device_data_[\"spn_fmi\"] = None\n",
    "    for index , (a, b) in enumerate(zip(device_data_[\"spn\"] , device_data_[\"fmi\"])):\n",
    "        device_data_.at[ index , \"spn_fmi\"] = (a,b)\n",
    "    device_data[i] = device_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcode_data[\"spn_fmi\"] = None\n",
    "for index , (a, b) in enumerate(zip(pcode_data[\"SPN\"] , pcode_data[\"FMI\"])):\n",
    "    pcode_data.at[ index , \"spn_fmi\"] = (a,b)\n",
    "\n",
    "code2pcode = {}\n",
    "for code , pcode in  zip(pcode_data[\"spn_fmi\"] , pcode_data[\"Error codes \"]):\n",
    "    code2pcode[code] = pcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(device_data)):\n",
    "    device_data[i][\"Pcode\"] = None\n",
    "    for index , spn_fmi_device in enumerate(device_data[i][\"spn_fmi\"] ):\n",
    "        if spn_fmi_device in code2pcode.keys():\n",
    "            device_data[i].at[index , \"Pcode\"] = code2pcode[spn_fmi_device]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(device_data)):\n",
    "    device_data[i].sort_values(\"utc\" , inplace=True , ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_interval = 30 # in mins\n",
    "time_interval = time_interval * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_pcodes_data = []\n",
    "targetErrorsML = []\n",
    "for i in range(len(device_data)):\n",
    "    temp = device_data[i].copy()\n",
    "    temp_list = []\n",
    "    temp_targetErrorsML = []\n",
    "    for index , pcode in enumerate(temp[\"Pcode\"]):\n",
    "        if pcode is not None:\n",
    "            utc = temp[\"utc\"].iloc[index]\n",
    "            mask = temp[\"utc\"].between(utc - time_interval , utc)\n",
    "            temp_list.append(temp[mask])\n",
    "            temp_targetErrorsML.append(pcode)\n",
    "    device_pcodes_data.append(temp_list)\n",
    "    targetErrorsML.append(temp_targetErrorsML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for i in range(len(device_data)):\n",
    "    vocab.update(device_data[i][\"spn_fmi\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2id = {}\n",
    "pcode2class = {}\n",
    "severe_codes = 0\n",
    "code2id[(-1 , -1)] = 0\n",
    "for i , voc in enumerate(vocab):\n",
    "    code2id[voc] = i + 1\n",
    "    if voc in code2pcode.keys() :\n",
    "        #code2id[voc] = code2id[voc] + 100000\n",
    "        pcode = code2pcode[voc]\n",
    "        pcode2class[pcode] = severe_codes + 1\n",
    "        severe_codes = severe_codes + 1\n",
    "id2code = {b:a for a,b in code2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2class = {}\n",
    "for code in code2id.keys():\n",
    "    code2class[code] = 0\n",
    "    if code in code2pcode:\n",
    "        code2class[code] = pcode2class[code2pcode[code]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ = []\n",
    "for device in targetErrorsML:\n",
    "    temp = []\n",
    "    for pcode in device:\n",
    "        c = pcode2class[pcode]\n",
    "        temp.append(c)\n",
    "    temp_.append(temp)\n",
    "targetErrorsML = temp_\n",
    "del temp\n",
    "del temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceDataML = []\n",
    "for device in device_pcodes_data:\n",
    "    temp_deviceDataML = []\n",
    "    for seq in device:\n",
    "        temp_seq = []\n",
    "        for code in seq[\"spn_fmi\"]:\n",
    "            temp_seq.append(code2id[code])\n",
    "        temp_deviceDataML.append(temp_seq)\n",
    "    deviceDataML.append(temp_deviceDataML)\n",
    "del temp_seq\n",
    "del temp_deviceDataML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_errors = []\n",
    "for i1 , device in enumerate(deviceDataML):\n",
    "    temp_list = []\n",
    "    for i2 , seq in enumerate(device):\n",
    "        temp = []\n",
    "        for element in seq:\n",
    "            temp.append(code2class[id2code[element]])\n",
    "        temp = temp[1:]\n",
    "        temp.append(targetErrorsML[i1][i2])\n",
    "        temp_list.append(temp)\n",
    "    target_errors.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_seq_lengths = []\n",
    "batch_lengths = []\n",
    "for i in range(len(ids)):\n",
    "    max_len = 0\n",
    "    for seq in deviceDataML[i]:\n",
    "        if max_len < len(seq):\n",
    "            max_len = len(seq)\n",
    "    batch_seq_lengths.append(max_len)\n",
    "    batch_lengths.append(len(deviceDataML[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextBatchID2(deviceID = 0 ):\n",
    "    max_len = batch_seq_lengths[deviceID]\n",
    "    seq_list = deviceDataML[deviceID]\n",
    "    targets = target_errors[deviceID]\n",
    "    #print(seq_list)\n",
    "    lengths = []\n",
    "    for seq in seq_list:\n",
    "        lengths.append(len(seq))\n",
    "    return \\\n",
    "            tf.keras.preprocessing.sequence.pad_sequences(seq_list , maxlen=max_len ,padding='post') , \\\n",
    "            tf.keras.preprocessing.sequence.pad_sequences(np.array(targets) , maxlen=max_len , padding='post') ,\\\n",
    "            lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchMask = np.array(batch_lengths) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch , y  , seq = getNextBatchID2(deviceID=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain  , ytrain = first_batch , y\n",
    "Xval , yval = None , None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 200\n",
    "n_layers = 1\n",
    "input_keep_prob = 0.8\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#tf.disable_eager_execution()\n",
    "# one device feeded once if len < 50 else 2 or more batches \n",
    "with tf.name_scope(\"Inputs\"):\n",
    "    input_seq_len = tf.placeholder(tf.int32 , shape=[None]  , name=\"seq_len\")\n",
    "    # Targets - either critical or No case\n",
    "    target_errors_placeholder = tf.placeholder(tf.int32 , shape=[None , None] , name=\"targets\")\n",
    "    inputs = tf.placeholder(tf.int32 , shape=[None , None] , name=\"inputs\")\n",
    "\n",
    "with tf.name_scope(\"Embeddings\"):\n",
    "    init_embeds = tf.random_uniform(shape= [ vocab_size , embed_size ]  , dtype=tf.float32)\n",
    "    embeddings = tf.Variable(init_embeds)\n",
    "    inputs_embeded = tf.nn.embedding_lookup(embeddings , inputs)\n",
    "\n",
    "with tf.name_scope(\"RNN\"):\n",
    "    basic_cell = [tf.contrib.rnn.GRUCell(num_units = n_neurons)] * n_layers\n",
    "    droupout_cell = [tf.contrib.rnn.DropoutWrapper(basic_cell_ , input_keep_prob=input_keep_prob) for basic_cell_ in basic_cell]\n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell(droupout_cell)\n",
    "    outputs , final_state = tf.nn.dynamic_rnn(stacked_cells , inputs_embeded , sequence_length=input_seq_len , dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"Hidden_Layers\"):\n",
    "    hidden_layer = tf.layers.dense(outputs , units=10 , activation=tf.nn.elu , kernel_initializer=tf.variance_scaling_initializer)\n",
    "    logits = tf.layers.dense(hidden_layer , severe_codes + 1 , activation=None , kernel_initializer=tf.variance_scaling_initializer())\n",
    "\n",
    "#final_state_concat = tf.concat(final_state , axis=1)\n",
    "#hidden_layer = tf.layers.dense(final_state_concat , units=10 , activation=tf.nn.elu , kernel_initializer=tf.variance_scaling_initializer())\n",
    "#logits = tf.layers.dense(hidden_layer , severe_codes + 1 , activation=None , kernel_initializer=tf.variance_scaling_initializer())\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_errors_placeholder , logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "with tf.name_scope(\"Evaluation\"):\n",
    "    predictions = tf.cast( tf.argmax(logits , axis=-1), tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast( tf.equal(predictions , target_errors_placeholder) , tf.float32))\n",
    "with tf.name_scope(\"Training\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients = optimizer.compute_gradients(loss , var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "    gradients = [ (tf.clip_by_value(var[0] , clip_value_max=1.5 , clip_value_min=-1.5)  , var[1] ) for var in gradients]\n",
    "    training_op = optimizer.apply_gradients(gradients)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope(\"Logs\"):\n",
    "    file_writer = tf.summary.FileWriter(\"tf_logs/\")\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions.get_shape()\n",
    "target_errors_placeholder.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 50\n",
    "num_examples =Xtrain.shape[0]\n",
    "batch_size = 10\n",
    "n_batches = num_examples // batch_size\n",
    "evaluation_gap = 1\n",
    "maxcheckswithoutprogress = 2000\n",
    "savedir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(sess):\n",
    "    return { gvar.op.name : sess.run(gvar) for gvar in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)}\n",
    "\n",
    "def restore_model_params(sess , params):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchMask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-eb17040d4b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                         }\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mevaluation_gap\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfeed_dict_val\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    file_writer.add_graph(tf.get_default_graph())\n",
    "    accuracy_val = 0\n",
    "    best_params = None\n",
    "    sess.run(init)\n",
    "    global_step = 0\n",
    "    feed_dict_val = None\n",
    "    if Xval is not None and yval is not None :\n",
    "        feed_dict_val = {inputs : Xval , target_reviews : yval , input_seq_len : seq_val}\n",
    "    best_yet = 0\n",
    "    checkswithoutprogress = 0\n",
    "    for epoch in range(n_epoch):\n",
    "        step = 0\n",
    "        #perms = np.random.permutation(num_examples)\n",
    "        #Xbatches = np.array_split(Xtrain[perms] , n_batches)\n",
    "        #ybatches = np.array_split(ytrain[perms] , n_batches)\n",
    "        for batch_id in range(len(ids)):\n",
    "            if batchMask[batch_id]:\n",
    "                break\n",
    "            #print(batch_id)\n",
    "            Xb , yb , seq = getNextBatchID2(deviceID=batch_id)\n",
    "            step = step + 1\n",
    "            global_step = global_step + 1\n",
    "            #print([batch_seq_lengths[batch_id]]*batch_lengths[batch_id])\n",
    "            batch_id = int(batch_id)\n",
    "            np.array([ batch_seq_lengths[batch_id] ] * batch_lengths[batch_id])\n",
    "            feed_dict = {\n",
    "                inputs : Xb ,\n",
    "                target_errors_placeholder : yb ,\n",
    "                input_seq_len : [batch_seq_lengths[batch_id]]*batch_lengths[batch_id] , \n",
    "                        }\n",
    "            print(batch_id)\n",
    "            sess.run(training_op , feed_dict = feed_dict)\n",
    "            if global_step % evaluation_gap == 0 :\n",
    "                if feed_dict_val is None :\n",
    "                    feed_dict_val = feed_dict\n",
    "                accuracy_val = sess.run(accuracy  , feed_dict=feed_dict_val)\n",
    "                if accuracy_val > best_yet:\n",
    "                    best_yet = accuracy_val\n",
    "                    best_params = get_model_params(sess)\n",
    "                    checkswithoutprogress = 0\n",
    "                else :\n",
    "                    checkswithoutprogress = checkswithoutprogress + 1\n",
    "            if checkswithoutprogress > maxcheckswithoutprogress :\n",
    "                print(\"Epoch {} Accuracy {} [Incomplete epoch , No progress for {} steps]\".format(epoch + 1 , accuracy_val , checkswithoutprogress))\n",
    "                break\n",
    "        else :\n",
    "            print(\"Epoch {} Accuracy {} \".format(epoch + 1 , accuracy_val))\n",
    "            continue\n",
    "        break\n",
    "    restore_model_params(sess , best_params)\n",
    "    saver.save(sess , savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    file_writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed_dict = {inputs : Xb , target_errors : yb }#,input_seq_len : [10]*50 }\n",
    "target_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[batch_seq_lengths[0]]*batch_lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
